% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/conta_main.R
\name{conta_main}
\alias{conta_main}
\title{Run conta.}
\usage{
conta_main(tsv_file, sample_id, save_dir, filename_prefix = sample_id,
  metrics_file = "", lr_th = 0.001, sim_level = 0,
  baseline_file = NA, min_depth = 10, max_depth = 10000,
  loh_lr_cutoff = 0.001, loh_delta_cutoff = 1.5,
  loh_auto_delta_cutoff = 9, loh_min_snps = 20, loh_max_snps = 1000,
  min_maf = 0.01, subsample = NA, cf_correction = 0,
  min_cf = 1e-04, blackswan = 1, outlier_frac = 0.002,
  tsv_rev_file = NA, cores = 2, context_mode = FALSE,
  chr_y_male_threshold = 5e-04, default_het_mean = 0.5,
  error_quantile_filter = 1, min_lr_to_cf_ratio = 0.1, seed = 1359)
}
\arguments{
\item{tsv_file}{input tsv file}

\item{save_dir}{output folder}

\item{filename_prefix}{experiment name (basename for outputs)}

\item{metrics_file}{input tsv metrics file in long format}

\item{lr_th}{min avg. likelihood ratio per SNP to make a call, this
number is highly dependent on the data type used and should be optimized
by the user based on a sensitivity - specificity trade-off.}

\item{sim_level}{if non-zero, a contaminant at this level will
be added to the current sample. The contaminant will be randomly
generated from minor allele frequencies of the SNPs in the population.}

\item{min_depth}{minimum depth for a SNP to be considered}

\item{max_depth}{maximum depth for a SNP to be considered}

\item{loh_lr_cutoff}{minimum likelihood ratio to call a region as LOH}

\item{loh_delta_cutoff}{minimum delta (het deviation) to call a region as LOH}

\item{loh_auto_delta_cutoff}{minimum delta to call a region automatically,
without looking at contamination likelihood, as LOH}

\item{loh_min_snps}{minimum number of SNPs in a region to consider LOH}

\item{loh_max_snps}{maxmimum number of SNPs in a region to use for LOH, if
there are more SNPs, they are subsampled to this number}

\item{min_maf}{minimum minor allele frequency to include a SNP}

\item{subsample}{Either NA (use all SNPs) or number of SNPs to subsample to}

\item{cf_correction}{cf correction calculated from empirical data}

\item{min_cf}{minimum contamination fraction to call}

\item{blackswan}{blackswan term for maximum likelihood estimation}

\item{outlier_frac}{fraction of outlier SNPs (based on depth) to remove}

\item{tsv_rev_file}{input tsv file for reverse strand reads, if this option
is provided then first tsv_file is considered as positive strand reads}

\item{cores}{number of cores to be used for parallelization}

\item{context_mode}{whether to run with errors calculated in 3-base context}

\item{default_het_mean}{default for heterozygote mean allele frequency}

\item{error_quantile_filter}{remove SNPs with error rates higher than this
quantile. Default 1.0 does not remove any SNPs. The idea is to remove
SNPs with high mean error rates, since they might also contain high
variance and thus introduce spurious contamination likelihoods.}

\item{min_lr_to_cf_ratio}{do not make a conta call if likelihood ratio to
contamination fraction is equal to or below this ratio. In general
conta likelihood ratio is greater than and is correlated with
the contamination fraction. In cases where this ratio is greatly violated
such as 10 times or less likelihood ratio compared to contamination
fraction, it is very likely that is is a spurious call generated due to
LOH or other artifacts.}

\item{seed}{random seed}

\item{baseline}{baseline file for blacklist or noise model}
}
\value{
none
}
\description{
Reads a given counts file, processes it and calculates
contamination likelihood for a set of ranges.
}
